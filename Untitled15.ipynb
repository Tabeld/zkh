{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Wo4fLBU5M0d8",
        "outputId": "2a9f3987-2425-4176-c4b4-07a1340afe64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting dawg-python>=0.7.1 (from pymorphy2)\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4 (from pymorphy2)\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting docopt>=0.6 (from pymorphy2)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=9eb6464a0dc2ce12d08dbe94cc7f7e093581b67324011f62abaa2be4c53f75b5\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
            "Successfully built docopt\n",
            "Installing collected packages: pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Collecting squarify\n",
            "  Downloading squarify-0.4.4-py3-none-any.whl.metadata (600 bytes)\n",
            "Downloading squarify-0.4.4-py3-none-any.whl (4.1 kB)\n",
            "Installing collected packages: squarify\n",
            "Successfully installed squarify-0.4.4\n",
            "Collecting faker\n",
            "  Downloading faker-37.5.3-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.11/dist-packages (from faker) (2025.2)\n",
            "Downloading faker-37.5.3-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faker\n",
            "Successfully installed faker-37.5.3\n",
            "Collecting keybert\n",
            "  Downloading keybert-0.9.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from keybert) (2.0.2)\n",
            "Requirement already satisfied: rich>=10.4.0 in /usr/local/lib/python3.11/dist-packages (from keybert) (13.9.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.11/dist-packages (from keybert) (1.6.1)\n",
            "Requirement already satisfied: sentence-transformers>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from keybert) (5.0.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.4.0->keybert) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.4.0->keybert) (2.19.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.2->keybert) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.2->keybert) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.2->keybert) (3.6.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.55.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.3.8->keybert) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.3.8->keybert) (0.34.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.3.8->keybert) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (1.1.7)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (0.6.2)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (0.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2025.8.3)\n",
            "Downloading keybert-0.9.0-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.4/41.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, keybert\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed keybert-0.9.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install pymorphy2\n",
        "!pip install nltk\n",
        "!pip install squarify\n",
        "!pip install faker\n",
        "!pip install keybert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfDkUNteMSvA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "from faker import Faker\n",
        "from datetime import datetime, timedelta\n",
        "import uuid\n",
        "import datetime\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from keybert import KeyBERT\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from umap import UMAP\n",
        "\n",
        "import seaborn as sns\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHuzzfyuNFjc"
      },
      "outputs": [],
      "source": [
        "# Загрузка данных\n",
        "df = pd.read_excel('L_corrected1.xlsx')\n",
        "\n",
        "# После завершения обработки сохраните результат\n",
        "#df.to_csv('L.csv', index=False, encoding='utf-8-sig')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3GuXxGzrNDVC"
      },
      "outputs": [],
      "source": [
        "# 1. Удаление полных дубликатов\n",
        "print(f\"Исходное количество строк: {len(df)}\")\n",
        "df = df.drop_duplicates()\n",
        "print(f\"После удаления дубликатов: {len(df)}\")\n",
        "\n",
        "# 2. Удаление пустых описаний\n",
        "df = df[df['Desc'].notna() & (df['Desc'].astype(str).str.strip() != '')]\n",
        "print(f\"После удаления пустых описаний: {len(df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1b_tA4XdliB"
      },
      "outputs": [],
      "source": [
        "# 1. Анализ распределения заявок\n",
        "plt.figure(figsize=(12, 6))\n",
        "ax = sns.countplot(data=df, y='Group', order=df['Group'].value_counts().index)\n",
        "plt.title('Распределение заявок по службам')\n",
        "plt.xlabel('Количество заявок')\n",
        "plt.ylabel('Служба')\n",
        "\n",
        "# Добавление аннотаций с количеством\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{int(p.get_width())}',\n",
        "                (p.get_width(), p.get_y() + p.get_height()/2.),\n",
        "                ha='left', va='center', xytext=(5, 0), textcoords='offset points')\n",
        "plt.tight_layout()\n",
        "plt.savefig('service_distribution.png', dpi=300)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fmqR6l_eLUQ"
      },
      "outputs": [],
      "source": [
        "# Списки ключевых слов для классификации\n",
        "water_supply_keywords = [\n",
        "    'хвс', 'гвс', 'водоснабжени', 'горячая вода', 'холодная вода',\n",
        "    'труба вод', 'напор воды', 'давление воды', 'нет воды', 'отсутствует вода',\n",
        "    'слабый напор', 'течет вода', 'протечка воды', 'счетчик воды', 'счётчик воды',\n",
        "    'трубопровод вод', 'водопровод', 'краны вод', 'кран вод', 'смеситель',\n",
        "    'бойлер', 'водонагреватель', 'змеевик', 'отсутствует хвс', 'отсутствует гвс',\n",
        "    'нет хвс', 'нет гвс', 'течь хвс', 'течь гвс', 'порыв трубы хвс',\n",
        "    'порыв трубы гвс', 'течь трубы хвс', 'течь трубы гвс'\n",
        "]\n",
        "\n",
        "water_drain_keywords = [\n",
        "    'канализац', 'засор', 'кнс', 'слив', 'сточные воды', 'водоотведен',\n",
        "    'запах канализации', 'забилась', 'протечка канализации', 'засорилась',\n",
        "    'затопление канализации', 'затопление кнс', 'ливневка', 'ливнёвка',\n",
        "    'ливневая канализация', 'дождевая канализация', 'стояк кнс', 'труба кнс'\n",
        "]\n",
        "\n",
        "electricity_keywords = [\n",
        "    'электричеств', 'электроэнерги', 'свет', 'лампочк', 'розетк', 'выключател',\n",
        "    'проводк', 'счетчик электро', 'счётчик электро', 'автомат', 'короткое замыкание',\n",
        "    'искрит', 'нет света', 'отсутствует свет', 'перебои с электричеством',\n",
        "    'эл.снабжение', 'эл.энергия', 'электрический', 'светильник', 'патрон',\n",
        "    'эл.щиток', 'электрощит', 'электропроводка', 'электролиния', 'электрокабель',\n",
        "    'свет отсутствует', 'свет не горит', 'не работает свет', 'отключение электричества',\n",
        "    'отключение света', 'электроэнергия отключена', 'электричество отключено',\n",
        "    'эл.счетчик', 'эл счетчик', 'электрический счетчик', 'электроэнерги',\n",
        "    'электричеств', 'счетчик электроэнергии', 'счётчик электроэнергии'\n",
        "]\n",
        "\n",
        "def classify_group(desc, cat):\n",
        "    combined_text = f\"{desc} {cat}\".lower()\n",
        "\n",
        "    for keyword in water_drain_keywords:\n",
        "        if keyword in combined_text:\n",
        "            return 'Водоотведение'\n",
        "\n",
        "    for keyword in water_supply_keywords:\n",
        "        if keyword in combined_text:\n",
        "            return 'Канализация'\n",
        "\n",
        "    for keyword in electricity_keywords:\n",
        "        if keyword in combined_text:\n",
        "            return 'Электроэнергия'\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "df_copy = df.copy()\n",
        "\n",
        "classified_groups = df_copy.apply(\n",
        "    lambda row: classify_group(row['Desc'], row['Cat']) if row['Group'] == 'Управление домом' else None,\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "\n",
        "df_copy.loc[classified_groups.notna(), 'Group'] = classified_groups[classified_groups.notna()]\n",
        "\n",
        "\n",
        "df_copy.to_csv('L_corrected.csv', index=False)\n",
        "print(\"Файл 'L_corrected.xlsx' успешно обновлен.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLsSVsSnipnP"
      },
      "outputs": [],
      "source": [
        "# Загрузка данных\n",
        "#df = pd.read_csv('L_corrected.csv', parse_dates=['Date'], dayfirst=False)\n",
        "\n",
        "# Словарь замены сокращений\n",
        "abbreviation_map = {\n",
        "    r'\\bХ/ГВС\\b': 'Холодная вода вместо горячей',\n",
        "    r'\\bГВС\\b': 'Горячее водоснабжение',\n",
        "    r'\\bХВС\\b': 'Холодное водоснабжение',\n",
        "    r'\\bЦО\\b': 'Центральное отопление',\n",
        "    r'\\bКНС\\b': 'Канализационный насосный станция',\n",
        "    r'\\bХ/З\\b': 'Холодно в зале',\n",
        "    r'\\bХ/Б\\b': 'Холодные батареи',\n",
        "    r'\\bАДС\\b': 'Аварийно-диспетчерская служба',\n",
        "    r'\\bДВК\\b': 'Дымовентиляционный канал',\n",
        "    r'\\bсм/б\\b': 'Сливной бачок',\n",
        "    r'\\bс/б\\b': 'Сливной бачок',\n",
        "    r'\\bс/у\\b': 'Санузел',\n",
        "    r'\\bраб.\\b': 'работает',\n",
        "    r'\\bэл\\. ?эн\\.?': 'Электроэнергия',\n",
        "    r'\\bИПУ\\b': 'Индивидуальный прибор учета',\n",
        "    r'\\bл/пл\\b': 'Лестничная площадка',\n",
        "    r'\\bкв\\.': 'Квартира',\n",
        "    r'\\bэт\\.': 'Этаж',\n",
        "    r'\\bул\\.': 'Улица',\n",
        "    r'\\bпод\\.': 'Подъезд',\n",
        "    r'\\bгр\\.': 'Группа',\n",
        "    r'\\bтреб\\.': 'Требуется',\n",
        "    r'\\bрем\\.': 'Ремонт',\n",
        "    r'\\bул\\.': 'Улица',\n",
        "    r'\\bпр\\.': 'Проспект',\n",
        "    r'\\bпер\\.': 'Переулок',\n",
        "    r'\\bобщ\\.': 'Общий',\n",
        "    r'\\bст\\.': 'Стояк',\n",
        "    r'\\bсв\\.': 'Свет',\n",
        "    r'\\bсч\\.': 'Счетчик',\n",
        "    r'\\bпломб\\.': 'пломбировать',\n",
        "    r'\\bотс\\.': 'Отсечной',\n",
        "    r'\\bвент\\.': 'Вентиляционный',\n",
        "    r'\\bсан\\.': 'Санитарный',\n",
        "    r'\\bэл\\.': 'Электрический',\n",
        "    r'\\bкаб\\.': 'Кабинет',\n",
        "    r'\\bкорп\\.': 'Корпус',\n",
        "    r'\\bд\\.': 'Дом',\n",
        "    r'\\bзал\\b': 'Зал',\n",
        "    r'\\bпротоп\\b': 'Протечка'\n",
        "}\n",
        "\n",
        "# Функция для замены сокращений с учетом границ слов\n",
        "def replace_abbreviations(text):\n",
        "    if not isinstance(text, str):\n",
        "        return text\n",
        "\n",
        "    for pattern, replacement in abbreviation_map.items():\n",
        "        text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)\n",
        "    return text\n",
        "\n",
        "# Применяем замену к целевым колонкам\n",
        "columns_to_process = ['Desc', 'Cat', 'Group']\n",
        "for col in columns_to_process:\n",
        "    df[col] = df[col].apply(replace_abbreviations)\n",
        "\n",
        "# Сохраняем результат\n",
        "df = df.drop(columns=[col for col in df.columns if 'Unnamed' in col])\n",
        "\n",
        "df.to_csv('decoded_dataset.csv', index=False, encoding='utf-8-sig')\n",
        "\n",
        "print(\"Обработка завершена. Результат сохранен в decoded_dataset.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 1. Анализ распределения заявок\n",
        "plt.figure(figsize=(12, 6))\n",
        "ax = sns.countplot(data=df_copy, y='Group', order=df_copy['Group'].value_counts().index)\n",
        "plt.title('Распределение заявок по службам')\n",
        "plt.xlabel('Количество заявок')\n",
        "plt.ylabel('Служба')\n",
        "\n",
        "# Добавление аннотаций с количеством\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{int(p.get_width())}',\n",
        "                (p.get_width(), p.get_y() + p.get_height()/2.),\n",
        "                ha='left', va='center', xytext=(5, 0), textcoords='offset points')\n",
        "plt.tight_layout()\n",
        "plt.savefig('service_distribution_new.png', dpi=300)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "k4tWgudjf096"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HK130U0dTOc"
      },
      "outputs": [],
      "source": [
        "TOP_N_GROUPS = 5 # Количество топ-групп для отображения\n",
        "TOP_N_CATEGORIES = 10 # Количество топ-категорий для отображения\n",
        "MAX_WORDS_WORDCLOUD = 100 # Максимальное количество слов в облаке\n",
        "\n",
        "\n",
        "#2. Общая информация о датасете\n",
        "print(\"Общая информация о датасете:\")\n",
        "print(f\"Всего записей: {len(df)}\")\n",
        "print(f\"Колонки: {', '.join(df.columns)}\")\n",
        "\n",
        "#3. Анализ по Группам ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Анализ по Группам:\")\n",
        "\n",
        "if 'Group' in df.columns:\n",
        "    unique_groups = df['Group'].nunique()\n",
        "    print(f\"Количество уникальных групп: {unique_groups}\")\n",
        "\n",
        "    # Распределение по всем группам\n",
        "    group_counts = df['Group'].value_counts()\n",
        "    print(\"\\nРаспределение по всем группам:\")\n",
        "    print(group_counts)\n",
        "\n",
        "    # Топ групп\n",
        "    top_groups = group_counts.head(TOP_N_GROUPS)\n",
        "    print(f\"\\nТоп-{TOP_N_GROUPS} групп:\")\n",
        "    print(top_groups)\n",
        "\n",
        "    # Визуализация распределения групп\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    top_groups.plot(kind='barh', color='purple') # Горизонтальная гистограмма для лучшей читаемости\n",
        "    plt.title(f'Топ-{TOP_N_GROUPS} групп заявок')\n",
        "    plt.xlabel('Количество заявок')\n",
        "    plt.ylabel('Группа')\n",
        "    plt.gca().invert_yaxis() # Чтобы самая частая группа была сверху\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 4. Анализ по Категориям\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Анализ по Категориям:\")\n",
        "\n",
        "if 'Cat' in df.columns:\n",
        "    unique_categories = df['Cat'].nunique()\n",
        "    print(f\"Количество уникальных категорий: {unique_categories}\")\n",
        "\n",
        "    category_counts = df['Cat'].value_counts()\n",
        "    print(f\"\\nТоп-{TOP_N_CATEGORIES} категорий:\")\n",
        "    top_categories = category_counts.head(TOP_N_CATEGORIES)\n",
        "    print(top_categories)\n",
        "\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    top_categories.plot(kind='barh', color='orange')\n",
        "    plt.title(f'Топ-{TOP_N_CATEGORIES} категорий заявок')\n",
        "    plt.xlabel('Количество заявок')\n",
        "    plt.ylabel('Категория')\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# 5. Анализ по Приоритетам\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Анализ по Приоритетам:\")\n",
        "\n",
        "if 'Priority' in df.columns:\n",
        "    unique_priorities = df['Priority'].nunique()\n",
        "    print(f\"Количество уникальных приоритетов: {unique_priorities}\")\n",
        "\n",
        "    # Распределение по приоритетам\n",
        "    priority_counts = df['Priority'].value_counts().sort_index() # Сортируем по индексу (названию приоритета)\n",
        "    print(\"\\nРаспределение по приоритетам:\")\n",
        "    print(priority_counts)\n",
        "\n",
        "    # Визуализация распределения приоритетов\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    priority_counts.plot(kind='pie', autopct='%1.1f%%', startangle=90, colors=sns.color_palette('pastel'))\n",
        "    plt.title('Распределение приоритетов заявок')\n",
        "    plt.ylabel('') # Убираем подпись оси Y для круговой диаграммы\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Столбец 'Priority' не найден, анализ приоритетов пропущен.\")\n",
        "\n",
        "# --- 6. Взаимосвязь Группы и Приоритета ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Взаимосвязь Группы и Приоритета:\")\n",
        "\n",
        "if 'Group' in df.columns and 'Priority' in df.columns:\n",
        "    # Создаем кросс-таблицу\n",
        "    priority_group_crosstab = pd.crosstab(df['Group'], df['Priority'])\n",
        "\n",
        "    if not priority_group_crosstab.empty:\n",
        "        print(\"\\nКросс-таблица: Группа vs Приоритет\")\n",
        "        print(priority_group_crosstab.head(10)) # Показываем первые 10 строк кросс-таблицы\n",
        "\n",
        "        # Визуализация\n",
        "        plt.figure(figsize=(16, 10))\n",
        "        priority_group_crosstab.plot(kind='bar', stacked=True, colormap='viridis')\n",
        "        plt.title('Распределение приоритетов по группам')\n",
        "        plt.xlabel('Группа')\n",
        "        plt.ylabel('Количество заявок')\n",
        "        plt.legend(title='Приоритет')\n",
        "        plt.xticks(rotation=90)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"Нет данных для построения кросс-таблицы 'Группа vs Приоритет'.\")\n",
        "else:\n",
        "    print(\"Столбцы 'Group' или 'Priority' не найдены, анализ взаимосвязи пропущен.\")\n",
        "\n",
        "# --- 7. Взаимосвязь Категории и Приоритета ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Взаимосвязь Категории и Приоритета:\")\n",
        "\n",
        "if 'Cat' in df.columns and 'Priority' in df.columns:\n",
        "    # Создаем кросс-таблицу, но ограничиваемся топ-категориями для лучшей визуализации\n",
        "    if 'Cat' in df.columns and not df['Cat'].isnull().all() and not top_categories.empty:\n",
        "        categories_to_plot = top_categories.index\n",
        "        df_filtered_cat = df[df['Cat'].isin(categories_to_plot)]\n",
        "\n",
        "        priority_cat_crosstab = pd.crosstab(df_filtered_cat['Cat'], df_filtered_cat['Priority'])\n",
        "\n",
        "        if not priority_cat_crosstab.empty:\n",
        "            print(f\"\\nКросс-таблица: Топ-{TOP_N_CATEGORIES} Категорий vs Приоритет\")\n",
        "            print(priority_cat_crosstab)\n",
        "\n",
        "            # Визуализация\n",
        "            plt.figure(figsize=(16, 10))\n",
        "            priority_cat_crosstab.plot(kind='bar', stacked=True, colormap='plasma')\n",
        "            plt.title(f'Распределение приоритетов по топ-{TOP_N_CATEGORIES} категориям')\n",
        "            plt.xlabel('Категория')\n",
        "            plt.ylabel('Количество заявок')\n",
        "            plt.legend(title='Приоритет')\n",
        "            plt.xticks(rotation=90)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(\"Нет данных для построения кросс-таблицы 'Категория vs Приоритет'.\")\n",
        "    else:\n",
        "        print(\"Недостаточно данных в категориях или столбец 'Cat' отсутствует для анализа взаимосвязи с приоритетом.\")\n",
        "else:\n",
        "    print(\"Столбцы 'Cat' или 'Priority' не найдены, анализ взаимосвязи пропущен.\")\n",
        "\n",
        "# --- 8. Анализ Текстовых Описаний (Облако Слов) ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Анализ текстовых описаний (Облако Слов):\")\n",
        "\n",
        "if 'Desc' in df.columns and not df['Desc'].isnull().all():\n",
        "    # Объединяем все описания в один текст, игнорируя пустые и NaN\n",
        "    text = \" \".join(str(desc) for desc in df['Desc'] if pd.notna(desc) and str(desc).strip())\n",
        "\n",
        "    if text: # Проверяем, не пуст ли итоговый текст\n",
        "        try:\n",
        "            wordcloud = WordCloud(width=800,\n",
        "                                  height=400,\n",
        "                                  background_color='white',\n",
        "                                  max_words=MAX_WORDS_WORDCLOUD,\n",
        "                                  collocations=False # Выключаем группировку слов, чтобы получить более точные результаты\n",
        "                                 ).generate(text)\n",
        "\n",
        "            plt.figure(figsize=(14, 8))\n",
        "            plt.imshow(wordcloud, interpolation='bilinear')\n",
        "            plt.axis(\"off\")\n",
        "            plt.title(f'Облако слов для описаний заявок (топ {MAX_WORDS_WORDCLOUD} слов)', fontsize=16)\n",
        "            plt.show()\n",
        "        except ValueError as ve:\n",
        "            print(f\"Не удалось создать облако слов: {ve}. Возможно, текст слишком короткий или содержит только стоп-слова.\")\n",
        "    else:\n",
        "        print(\"Нет данных для создания облака слов (описания пусты или отсутствуют).\")\n",
        "else:\n",
        "    print(\"Столбец 'Desc' не найден или пуст, анализ описаний пропущен.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Анализ завершен.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Настройка стиля графиков\n",
        "sns.set(style=\"whitegrid\", palette=\"pastel\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "\n",
        "# 1. Предварительный анализ данных\n",
        "print(\"=\"*50)\n",
        "print(\"Основная информация о датасете:\")\n",
        "print(f\"Всего записей: {len(df)}\")\n",
        "print(f\"Колонки: {', '.join(df.columns)}\")\n",
        "print(\"\\nПропущенные значения:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# 3. Анализ приоритетов\n",
        "priority_counts = df['Priority'].value_counts().sort_index()\n",
        "\n",
        "# 4. Анализ по группам и категориям\n",
        "group_counts = df['Group'].value_counts()\n",
        "top_categories = df['Cat'].value_counts().head(10)\n",
        "\n",
        "# 5. Визуализация\n",
        "plt.figure(figsize=(14, 16))\n",
        "\n",
        "# Распределение приоритетов\n",
        "plt.subplot(3, 2, 4)\n",
        "priority_counts.plot(kind='pie', autopct='%1.1f%%', colors=['tomato', 'gold', 'lightblue', 'limegreen'])\n",
        "plt.title('Распределение приоритетов заявок')\n",
        "plt.ylabel('')\n",
        "\n",
        "# Топ групп\n",
        "plt.subplot(3, 2, 5)\n",
        "group_counts.plot(kind='barh', color='purple')\n",
        "plt.title('Распределение заявок по группам')\n",
        "plt.xlabel('Количество заявок')\n",
        "plt.ylabel('Группа')\n",
        "\n",
        "# Топ категорий\n",
        "plt.subplot(3, 2, 6)\n",
        "top_categories.plot(kind='barh', color='orange')\n",
        "plt.title('Топ-10 категорий заявок')\n",
        "plt.xlabel('Количество заявок')\n",
        "plt.ylabel('Категория')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 6. Дополнительный анализ: взаимосвязь приоритета и групп\n",
        "plt.figure(figsize=(14, 8))\n",
        "priority_group = pd.crosstab(df['Group'], df['Priority'])\n",
        "priority_group.plot(kind='bar', stacked=True)\n",
        "plt.title('Распределение приоритетов по группам')\n",
        "plt.xlabel('Группа')\n",
        "plt.ylabel('Количество заявок')\n",
        "plt.legend(title='Приоритет')\n",
        "plt.show()\n",
        "\n",
        "# 7. Анализ текстовых описаний\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Объединяем все описания в один текст\n",
        "text = \" \".join(desc for desc in df['Desc'].astype(str))\n",
        "\n",
        "# Создаем облако слов\n",
        "wordcloud = WordCloud(width=800,\n",
        "                      height=400,\n",
        "                      background_color='white',\n",
        "                      max_words=100).generate(text)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.title('Облако слов для описаний заявок', fontsize=16)\n",
        "plt.show()\n",
        "\n",
        "# 8. Статистика по времени реакции\n",
        "print(\"\\nСтатистика по времени реакции:\")\n",
        "print(f\"Максимальный приоритет: {df['Priority'].min()}\")\n",
        "print(f\"Минимальный приоритет: {df['Priority'].max()}\")\n",
        "print(f\"Средний приоритет: {df['Priority'].mean():.2f}\")\n",
        "\n",
        "# Вывод дополнительной статистики\n",
        "print(\"\\nТоп-5 самых частых проблем:\")\n",
        "print(top_categories.head(5))\n",
        "print(\"\\nТоп-5 самых активных групп:\")\n",
        "print(group_counts.head(5))"
      ],
      "metadata": {
        "id": "XI2H91kWo2Kv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HO2rqKTdOX86"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud\n",
        "import nltk\n",
        "from string import punctuation\n",
        "\n",
        "# Настройки визуализации\n",
        "sns.set(style=\"whitegrid\", palette=\"pastel\", font_scale=1.2)\n",
        "plt.rcParams['figure.figsize'] = (14, 8)\n",
        "plt.rcParams['font.family'] = 'DejaVu Sans'  # Для поддержки кириллицы\n",
        "\n",
        "# Загрузка данных\n",
        "df = pd.read_csv('prioritized_dataset.csv')\n",
        "\n",
        "# Удаление пустых столбцов\n",
        "df = df.drop(columns=[col for col in df.columns if 'Unnamed' in col])\n",
        "\n",
        "# 1. Предобработка текста заявок\n",
        "nltk.download('stopwords')\n",
        "russian_stopwords = stopwords.words('russian')\n",
        "additional_stopwords = ['заявка', 'добрый', 'вечер', 'день', 'пожалуйста', 'прошу', 'уважаемый',\n",
        "                        'сделать', 'очень', 'нужно', 'нужна', 'нужен', 'нужно', 'нужны']\n",
        "russian_stopwords.extend(additional_stopwords)\n",
        "\n",
        "def clean_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    # Приведение к нижнему регистру\n",
        "    text = text.lower()\n",
        "    # Удаление спецсимволов и цифр\n",
        "    text = re.sub(r'[^а-яё\\s]', '', text)\n",
        "    # Удаление стоп-слов\n",
        "    words = text.split()\n",
        "    words = [word for word in words if word not in russian_stopwords and len(word) > 2]\n",
        "    return \" \".join(words)\n",
        "\n",
        "df['Cleaned_Desc'] = df['Desc'].apply(clean_text)\n",
        "\n",
        "# 2. Анализ текста заявок\n",
        "# Собираем все слова в один список\n",
        "all_words = []\n",
        "for desc in df['Cleaned_Desc']:\n",
        "    all_words.extend(desc.split())\n",
        "\n",
        "# Частотное распределение слов\n",
        "word_freq = Counter(all_words)\n",
        "common_words = word_freq.most_common(20)\n",
        "\n",
        "# Создаем DataFrame для топ-20 слов\n",
        "top_words_df = pd.DataFrame(common_words, columns=['Word', 'Count'])\n",
        "\n",
        "# 3. Визуализация анализа текста\n",
        "plt.figure(figsize=(16, 20))\n",
        "\n",
        "# Топ-20 слов в описаниях\n",
        "plt.subplot(3, 2, 1)\n",
        "sns.barplot(x='Count', y='Word', data=top_words_df, palette='viridis')\n",
        "plt.title('Топ-20 слов в описаниях заявок')\n",
        "plt.xlabel('Количество упоминаний')\n",
        "plt.ylabel('Слово')\n",
        "\n",
        "# Распределение длины описаний\n",
        "df['Desc_Length'] = df['Cleaned_Desc'].apply(lambda x: len(x.split()))\n",
        "plt.subplot(3, 2, 2)\n",
        "sns.histplot(df['Desc_Length'], bins=30, kde=True, color='skyblue')\n",
        "plt.title('Распределение длины описаний')\n",
        "plt.xlabel('Количество слов в описании')\n",
        "plt.ylabel('Количество заявок')\n",
        "plt.xlim(0, 50)  # Ограничим для лучшей визуализации\n",
        "\n",
        "# Облако слов для всех описаний\n",
        "plt.subplot(3, 2, 3)\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white',\n",
        "                      max_words=100).generate_from_frequencies(word_freq)\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Облако слов для описаний заявок')\n",
        "\n",
        "# Топ-10 групп проблем\n",
        "plt.subplot(3, 2, 4)\n",
        "group_counts = df['Group'].value_counts().head(10)\n",
        "sns.barplot(x=group_counts.values, y=group_counts.index, palette='rocket')\n",
        "plt.title('Топ-10 групп проблем')\n",
        "plt.xlabel('Количество заявок')\n",
        "plt.ylabel('Группа')\n",
        "\n",
        "# Топ-15 категорий проблем\n",
        "plt.subplot(3, 2, 5)\n",
        "category_counts = df['Cat'].value_counts().head(15)\n",
        "sns.barplot(x=category_counts.values, y=category_counts.index, palette='mako')\n",
        "plt.title('Топ-15 категорий проблем')\n",
        "plt.xlabel('Количество заявок')\n",
        "plt.ylabel('Категория')\n",
        "\n",
        "# Распределение приоритетов\n",
        "plt.subplot(3, 2, 6)\n",
        "priority_counts = df['Priority'].value_counts().sort_index()\n",
        "priority_labels = ['Экстренная', 'Критическая', 'Стандартная', 'Низкий']\n",
        "plt.pie(priority_counts, labels=priority_labels, autopct='%1.1f%%',\n",
        "        colors=['#ff9999','#66b3ff','#99ff99','#ffcc99'])\n",
        "plt.title('Распределение заявок по приоритетам')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 4. Анализ взаимосвязей\n",
        "plt.figure(figsize=(16, 10))\n",
        "\n",
        "# Приоритет по группам проблем\n",
        "plt.subplot(2, 2, 1)\n",
        "priority_by_group = df.groupby(['Group', 'Priority']).size().unstack()\n",
        "priority_by_group = priority_by_group.div(priority_by_group.sum(axis=1), axis=0)\n",
        "priority_by_group.plot(kind='barh', stacked=True, colormap='viridis')\n",
        "plt.title('Распределение приоритетов по группам проблем')\n",
        "plt.xlabel('Доля приоритетов')\n",
        "plt.ylabel('Группа проблем')\n",
        "plt.legend(title='Приоритет', labels=priority_labels)\n",
        "\n",
        "# Топ слов по приоритетам\n",
        "for i, priority in enumerate(priority_counts.index, 2):\n",
        "    plt.subplot(2, 2, i)\n",
        "    priority_df = df[df['Priority'] == priority]\n",
        "    priority_words = []\n",
        "    for desc in priority_df['Cleaned_Desc']:\n",
        "        priority_words.extend(desc.split())\n",
        "    word_freq_priority = Counter(priority_words).most_common(10)\n",
        "    priority_words_df = pd.DataFrame(word_freq_priority, columns=['Word', 'Count'])\n",
        "\n",
        "    sns.barplot(x='Count', y='Word', data=priority_words_df, palette='cool')\n",
        "    plt.title(f'Топ-10 слов для приоритета: {priority_labels[priority-1]}')\n",
        "    plt.xlabel('Количество упоминаний')\n",
        "    plt.ylabel('')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 5. Анализ уникальных слов в группах\n",
        "top_groups = df['Group'].value_counts().head(3).index\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, group in enumerate(top_groups, 1):\n",
        "    plt.subplot(2, 2, i)\n",
        "    group_df = df[df['Group'] == group]\n",
        "    group_words = []\n",
        "    for desc in group_df['Cleaned_Desc']:\n",
        "        group_words.extend(desc.split())\n",
        "\n",
        "    # Сравниваем с общим распределением\n",
        "    group_word_freq = Counter(group_words)\n",
        "    unique_words = {word: count for word, count in group_word_freq.items()\n",
        "                   if word_freq[word] == count}\n",
        "\n",
        "    if not unique_words:\n",
        "        common = Counter(group_words).most_common(10)\n",
        "        unique_words_df = pd.DataFrame(common, columns=['Word', 'Count'])\n",
        "        title = f'Топ-10 слов для \"{group}\" (нет уникальных)'\n",
        "    else:\n",
        "        unique_words_df = pd.DataFrame(unique_words.items(), columns=['Word', 'Count']).sort_values('Count', ascending=False).head(10)\n",
        "        title = f'Уникальные слова для \"{group}\"'\n",
        "\n",
        "    sns.barplot(x='Count', y='Word', data=unique_words_df, palette='flare')\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Количество упоминаний')\n",
        "    plt.ylabel('')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 6. Дополнительная статистика\n",
        "print(\"\\nДополнительная статистика:\")\n",
        "print(f\"Всего уникальных слов: {len(word_freq)}\")\n",
        "print(f\"Средняя длина описания: {df['Desc_Length'].mean():.1f} слов\")\n",
        "print(f\"Медианная длина описания: {df['Desc_Length'].median()} слов\")\n",
        "print(\"\\nСамые частые проблемы:\")\n",
        "print(df['Cat'].value_counts().head(10))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}